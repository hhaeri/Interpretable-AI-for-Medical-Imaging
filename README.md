# Interpretable AI for Medical Imaging Data

The objective of this project is to provide interpretable machine and deep learning methods for medical imaging data. In safety-critical applications, professionals often exhibit reluctance to rely on neural networks when these networks lack readily interpretable explanations. This hesitation stems from the need for a clear understanding of the network's decision-making processes to ensure safety and accountability.

This work aims to bridge this gap by emphasizing the importance of human interpretability. It strives to enable experts, such as medical practitioners, to actively participate in the decision-making process. By facilitating visual interpretations of the learned concepts within the network, this approach not only enhances transparency but also ensures that domain experts can provide valuable insights and oversight, further bolstering the safety and reliability of critical systems.

In this study I am using the [NIH chest Xray](https://nihcc.app.box.com/v/ChestXray-NIHCC/) dataset but the model is applicable to any other images and applications aiming to provide interpretable predictions. I am currently working to improve a novel VAE architecture called CLAP (**C**oncept **L**earning **A**nd **P**rediction) which aims to use visually interpretable concepts as predictor for a simple classifier. 

In this research, the [NIH chest X-ray dataset](https://nihcc.app.box.com/v/ChestXray-NIHCC/) serves as the basis for our investigation. However, it is essential to note that the model's utility extends beyond this specific dataset, as it can be applied to various other image-based applications with a focus on delivering interpretable predictions. Our current research efforts are directed towards the refinement of multiple regularization techniques tailored for enhancing the performance of a novel VAE architecture named [CLAP](https://arxiv.org/abs/2204.00492) (**C**oncept **L**earning **A**nd **P**rediction). The CLAP architecture is designed to harness visually interpretable concepts as predictor for a simple classifier.
